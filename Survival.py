from Dependencies import *
from Nervous_system import *
# ==========================================
# 0. ç¯å¢ƒé€‚é…å™¨
# ==========================================
class MyoAdapter:
    def __init__(self, env_name):
        self.env = myo_gym.make(env_name)
        self.env.reset()
        self.obs_dim = self.env.observation_space.shape[0]
        self.act_dim = self.env.action_space.shape[0]
        
    def reset(self):
        obs = self.env.reset()
        if isinstance(obs, tuple):
            return obs[0]
        return obs

    def step(self, action):
        step_result = self.env.step(action)
        if len(step_result) == 4:
            obs, reward, done, info = step_result
            truncated = False 
            return obs, reward, done, truncated, info
        else:
            return step_result 

    def render_live(self):
        try:
            self.env.mj_render()
        except Exception:
            pass 

    def render_frame(self, flip_vertical=False):
        # ğŸ¥ ç¦»å±æ¸²æŸ“ç”¨äºé€šè¿‡è§†é¢‘ä¿å­˜
        rgb = self.env.sim.renderer.render_offscreen(width=640, height=480, camera_id=-1)
        if flip_vertical:
            return np.flipud(rgb)
        return rgb

    def close(self):
        self.env.close()
# ==========================================
# 1. å·¥å…·å‡½æ•°ï¼šGAE è®¡ç®—
# ==========================================
def compute_gae(rewards, values, mask, next_value, gamma=0.99, lam=0.95):
    """
    è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ (Generalized Advantage Estimation)
    """
    returns = []
    gae = 0
    # å°† next_value åŠ å…¥ values åˆ—è¡¨æœ«å°¾æ–¹ä¾¿è®¡ç®—
    values = values + [next_value] 
    
    for i in reversed(range(len(rewards))):
        # TD Error (delta) = r + gamma * V(s') * mask - V(s)
        delta = rewards[i] + gamma * values[i + 1] * mask[i] - values[i]
        # GAE = delta + gamma * lambda * mask * GAE(prev)
        gae = delta + gamma * lam * mask[i] * gae
        # Return = GAE + V(s)
        returns.insert(0, gae + values[i])
    
    return returns
# ==========================================
# 2. SurvivalEngine
# ==========================================
class SurvivalEngine:
    """
    ç”Ÿå­˜å¼•æ“ï¼šè´Ÿè´£ä¸ç¯å¢ƒäº¤äº’ã€æ•°æ®é‡‡é›†ã€è¯„ä¼°è¡¨ç°
    ç±»æ¯”åŠ¨ç‰©çš„ã€Œèº«ä½“ã€å’Œã€Œä¸–ç•Œäº¤äº’æ¥å£ã€ï¼Œä¸åŒ…å«å­¦ä¹ é€»è¾‘
    """
    def __init__(self, env_name, brain, run_name="BioBrain_Survival"):
        self.brain = brain  # æ³¨å…¥ BioBrain å®ä¾‹
        self.device = next(brain.parameters()).device
        
        # ç¯å¢ƒä¸æ•°æ®ç¼“å†²
        self.env = MyoAdapter(env_name)
        self.obs = self.env.reset()
        self.rnn_hidden = None
        
        # è®°å½•ä¸å­˜å‚¨
        self.run_name = run_name
        self.writer = SummaryWriter(f"runs/{run_name}")
        self.ckpt_dir = f"checkpoints/{run_name}"
        self.video_dir = f"videos/{run_name}"
        os.makedirs(self.ckpt_dir, exist_ok=True)
        os.makedirs(self.video_dir, exist_ok=True)
        
        self.global_step = 0
        self.best_reward = -float('inf')
        self.load_checkpoint()

    # === ç¯å¢ƒäº¤äº’æ ¸å¿ƒ ===
    def collect_experience(self, batch_size=4096):
        """é‡‡é›†ä¸€æ‰¹ç»éªŒï¼ˆå¯¹åº”Agentçš„ä¸€æ¬¡ã€Œæ¢ç´¢è¿‡ç¨‹è®°å½•ã€ï¼‰"""
        buffers = {
            'obs': [], 'raw_intent': [], 'logp': [], 
            'value': [], 'reward': [], 'mask': [], 'hidden': [],
            'next_obs': [], 
        }
        
        steps = 0
        while steps < batch_size:
            # 1. å†³ç­–ï¼šè·å–åŠ¨ä½œ
            obs_t = torch.FloatTensor(self.obs).unsqueeze(0).to(self.device)
            with torch.no_grad():
                # âœ… é€‚é… forward è¿”å›å€¼ï¼šæ–°å¢ sensory_stateï¼ˆä½†å½“å‰ä¸éœ€è¦ï¼Œç”¨ _ å ä½ï¼‰
                act, logp, val, entropy, next_hidden, raw_intent, sensory_state = self.brain(
                    obs_t, hidden=self.rnn_hidden
                )

            # 2. æ‰§è¡ŒåŠ¨ä½œ
            next_obs, reward, done, trunc, _ = self.env.step(act.cpu().numpy().flatten())
            # å¤„ç† done å’Œ trunc çš„æƒ…å†µï¼Œè¿™äº›æƒ…å†µéœ€è¦é‡ç½®ç¯å¢ƒå’Œç½‘ç»œéšè—çŠ¶æ€ã€‚ä½†æ˜¯åé¢æˆ‘ä»¬ä¼šåŒºåˆ†doneå’Œtruncä¸¤ç§æƒ…å†µçš„ä»·å€¼è®¡ç®—
            is_terminal = done or trunc
            
            # 3. å­˜å‚¨ç»éªŒ
            buffers['obs'].append(self.obs)
            buffers['raw_intent'].append(raw_intent.cpu().numpy())
            buffers['logp'].append(logp.item())
            buffers['value'].append(val.item())
            buffers['reward'].append(reward)
            # å¦‚æœæ˜¯doneçš„æƒ…å†µï¼Œåˆ™è¦åœæ­¢åŠ ä¸Šä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼ï¼Œæ‰€ä»¥éœ€è¦ä¸ªmaskæ¥åˆ¤æ–­
            buffers['mask'].append(0.0 if done else 1.0)
            # å› ä¸ºä¿å­˜äº†next_obsï¼Œæ‰€ä»¥å¦‚æœæ˜¯truncatedæƒ…å†µï¼Œä¹Ÿèƒ½åƒå…¶ä»–æ­£å¸¸æƒ…å†µæŠŠä¸‹ä¸€ä¸ªçŠ¶æ€ä»·å€¼åŠ ä¸Š
            buffers['next_obs'].append(next_obs)
            # å­˜å‚¨å½“å‰çš„ hidden ç”¨äºä¹‹åæ¢¯åº¦è®¡ç®—æ—¶çš„åˆå§‹çŠ¶æ€
            current_h = self.rnn_hidden if self.rnn_hidden is not None else torch.zeros(1, 1, self.brain.config.sensory_latent).to(self.device)
            buffers['hidden'].append(current_h) 
            
            # 4. çŠ¶æ€æ›´æ–°
            self.obs = next_obs if not is_terminal else self.env.reset()
            self.rnn_hidden = next_hidden if not is_terminal else None
            self.global_step += 1
            steps += 1

        # 5. å¤„ç†ç»éªŒï¼ˆè®¡ç®— GAE å’Œ returnsï¼‰
        return self._process_buffers(buffers)

    def _process_buffers(self, buffers):
        """å°†åŸå§‹ç»éªŒè½¬æ¢ä¸ºè®­ç»ƒç”¨çš„ batchï¼ˆå« GAE è®¡ç®—ï¼‰"""
        
        # 1. è®¡ç®—æœ€åä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼ (Bootstrap Value)
        # åªæœ‰åœ¨ collect_experience å®Œæˆåæ‰è¿›è¡Œ
        with torch.no_grad():
            last_obs = torch.FloatTensor(buffers['obs'][-1]).unsqueeze(0).to(self.device)
            
            # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šå¦‚æœæœ¬è½®ç»“æŸæ—¶æ²¡æœ‰ doneï¼Œæˆ‘ä»¬å°±ç”¨æœ€åçš„ rnn_hiddenï¼›å¦‚æœ done äº†ï¼Œç†è®ºä¸Šä»·å€¼æ˜¯0ï¼Œè¿™é‡Œç”¨ None æˆ–å…¨0å‘é‡éƒ½è¡Œ
            last_hidden = self.rnn_hidden 
            
            _, _, next_val, _, _, _, _ = self.brain(last_obs, hidden=last_hidden)
            next_val = next_val.item()
        
        # 2. è°ƒç”¨å·¥å…·å‡½æ•°è®¡ç®— GAE
        returns = compute_gae(buffers['reward'], buffers['value'], buffers['mask'], next_val)
        
        # 3. å¤„ç† Hidden States æ‹¼æ¥
        hidden_batch = torch.cat(buffers['hidden'], dim=1).to(self.device)

        # 4. ç»„è£… Batch
        batch = {
            'obs': torch.FloatTensor(np.array(buffers['obs'])).to(self.device),
            'raw_intent': torch.FloatTensor(np.array(buffers['raw_intent'])).to(self.device),
            'logp': torch.FloatTensor(np.array(buffers['logp'])).to(self.device),
            'value': torch.FloatTensor(np.array(buffers['value'])).to(self.device),
            'return': torch.FloatTensor(np.array(returns)).to(self.device),
            'advantage': (torch.FloatTensor(np.array(returns)) - torch.FloatTensor(np.array(buffers['value']))).to(self.device),
            'hidden': hidden_batch,
            'next_obs': torch.FloatTensor(np.array(buffers['next_obs'])).to(self.device),
            'mask': torch.FloatTensor(np.array(buffers['mask'])).unsqueeze(1).to(self.device)
        }
        
        # Advantage æ ‡å‡†åŒ–ï¼ˆè®©è®­ç»ƒæ›´ç¨³å®šï¼‰ï¼Œç”¨äºactor-criticæ¶æ„ä¸‹çš„actoréƒ¨åˆ†ï¼Œæ‰€ä»¥ä¸ä¼šå½±å“criticçš„ç»å¯¹ä»·å€¼å­¦ä¹ 
        batch['advantage'] = (batch['advantage'] - batch['advantage'].mean()) / (batch['advantage'].std() + 1e-8)
        
        return batch

    # === è¯„ä¼°ä¸è®°å½• ===
    def evaluate_performance(self):
        """è¯„ä¼°å½“å‰ç­–ç•¥è¡¨ç°ï¼ˆæ— æ¢ç´¢ï¼‰"""
        obs = self.env.reset()
        hidden = None
        total_reward = 0.0
        
        while True:
            obs_t = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
            action, hidden = self.brain.get_action_deterministic(obs_t, hidden)
            obs, r, done, trunc, _ = self.env.step(action)
            total_reward += r
            if done or trunc:
                break
        
        return total_reward

    def save_video(self, max_steps=400):
        """å½•åˆ¶å½“å‰ç­–ç•¥çš„è§†é¢‘ï¼ˆæ— æ¢ç´¢ï¼‰"""
        video_path = f"{self.video_dir}/step_{self.global_step}.mp4"
        frames = []
        obs = self.env.reset()
        hidden = None
        
        for _ in range(max_steps):
            obs_t = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
            action, hidden = self.brain.get_action_deterministic(obs_t, hidden)
            obs, _, done, trunc, _ = self.env.step(action)
            frames.append(self.env.render_frame(flip_vertical=False))
            if done or trunc:
                break
        
        if frames:
            imageio.mimsave(video_path, frames, fps=30)
            print(f"ğŸ“¹ è§†é¢‘ä¿å­˜: {video_path}")

    # ===  checkpoint ç®¡ç† ===
    def save_checkpoint(self, score):
        is_best = score > self.best_reward
        if is_best:
            self.best_reward = float(score)

        payload = {
            "brain_state": self.brain.state_dict(),
            "optimizers": {k: v.state_dict() for k, v in self.brain.optimizers.items()},
            "step": int(self.global_step),
            "best_reward": float(self.best_reward),
        }
        torch.save(payload, f"{self.ckpt_dir}/{'best' if is_best else 'latest'}_checkpoint.pth")

        if is_best:
            print(f"ğŸ† æ–°çºªå½•: {score:.2f} (å·²ä¿å­˜)")

    def load_checkpoint(self):
        ckpt_path = glob.glob(f"{self.ckpt_dir}/*.pth")
        if not ckpt_path:
            print("âš ï¸ æ— æ£€æŸ¥ç‚¹å¯åŠ è½½")
            return

        latest_ckpt = max(ckpt_path, key=os.path.getctime)
        ckpt = torch.load(latest_ckpt, map_location=self.device, weights_only=False)

        self.brain.load_state_dict(ckpt["brain_state"], strict=False)

        saved_opts = ckpt.get("optimizers", {})
        for name, opt in self.brain.optimizers.items():
            state = saved_opts.get(name, None)
            if state is None:
                print(f"âš ï¸ optimizer[{name}] not found in ckpt, skip.")
                continue
            try:
                opt.load_state_dict(state)
            except ValueError as e:
                print(f"âš ï¸ optimizer[{name}] state mismatch, skip. ({e})")

        self.global_step = int(ckpt.get("step", 0))
        self.best_reward = float(ckpt.get("best_reward", -float("inf")))
        print(f"ğŸ”„ æ¢å¤æ£€æŸ¥ç‚¹: {latest_ckpt} (step {self.global_step})")